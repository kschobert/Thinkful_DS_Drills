{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Random Forest to Decision Trees -- Kristofer Schobert\n",
    "\n",
    "Here, we will be using a dataset from kaggle to predict if a patient has heart disease. We will first use a Random Forest Classifier to do so. Then try to beat it's cross validation score with a single desicion tree. We will also compare the runtimes of each model to get a sence of the computational demand of each model. \n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://www.kaggle.com/ronitf/heart-disease-uci/version/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first few rows of the data\n",
    "# The final column target is our output variable.\n",
    "# It has values of 0 (paitent does not have heart disease) and 1 (paitent has heart disease).\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the outcome variable from the rest of the data.\n",
    "# X is our model's input variables.\n",
    "# Y is the outcome variable.\n",
    "X = df.drop('target', 1)\n",
    "Y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "We will now run the random forest classifier. The output of the model will be different each time. The individual trees of the forest are made of randomly selected data from our dataset and randomly selected features for each branch.\n",
    "\n",
    "Let's run the model ten times. We will observe the runtime for the total of these ten models, and the accuracy of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.12)\n",
      "Accuracy: 0.81 (+/- 0.10)\n",
      "Accuracy: 0.81 (+/- 0.14)\n",
      "Accuracy: 0.79 (+/- 0.16)\n",
      "Accuracy: 0.80 (+/- 0.17)\n",
      "Accuracy: 0.82 (+/- 0.13)\n",
      "Accuracy: 0.82 (+/- 0.12)\n",
      "Accuracy: 0.82 (+/- 0.16)\n",
      "Accuracy: 0.81 (+/- 0.10)\n",
      "Accuracy: 0.80 (+/- 0.15)\n",
      "\n",
      "--- 1.4997000694274902 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Creating a for loop to run the model 10 times\n",
    "# Calculating the runtime for the 10 random forest models\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    # rfc will be the name of our random forest classifier\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    \n",
    "    # We will examine the mean and 2*sigma value of the cross validation scores \n",
    "    # for each of the 10 iterations of model. \n",
    "    score = cross_val_score(rfc, X, Y, cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "    \n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Random Forest Classifier seems to have a roughly 80% accuracy as it's mean. It does require a rather long amount of time for a dataset containing only 300 rows. \n",
    "\n",
    "Let's see if we can create a Decision Tree that has a similar or better accuracy. We will also compare it's runtime to that of the Random Forest above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.13)\n",
      "Accuracy: 0.76 (+/- 0.12)\n",
      "Accuracy: 0.79 (+/- 0.14)\n",
      "Accuracy: 0.78 (+/- 0.14)\n",
      "Accuracy: 0.79 (+/- 0.11)\n",
      "Accuracy: 0.79 (+/- 0.13)\n",
      "Accuracy: 0.79 (+/- 0.09)\n",
      "Accuracy: 0.78 (+/- 0.09)\n",
      "Accuracy: 0.76 (+/- 0.10)\n",
      "Accuracy: 0.76 (+/- 0.12)\n",
      "\n",
      "--- 0.3110029697418213 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Creating a for loop to run the model 10 times\n",
    "# Calculating the runtime for the 10 decision trees\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    # we will at first use default criteria for sklearns decision tree classifier \n",
    "    decision_tree = tree.DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        #max_features=1,\n",
    "        #max_depth=1,\n",
    "    )\n",
    "    \n",
    "    # We will examine the mean and 2*sigma value of the cross validation scores \n",
    "    # for each of the 10 iterations of model. \n",
    "    \n",
    "    score = cross_val_score(decision_tree, X, Y, cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total accuracy of these decision trees is close but not as great as the random forest's. The mean accuracy over the ten models here is roughly 78%. None, of the ten decision trees have a mean accuracy at or above 80%. Also, the variances of these desicion trees are a bit less than the random forests. \n",
    "\n",
    "The runtime for the decision trees is far less than that for the random forests. \n",
    "\n",
    "We chose the criteria of entropy gain to be the deciding factor when spliting.\n",
    "\n",
    "The default for max_features (the number of features that may be chosen from when forming a split) is to use all the total number of features. \n",
    "\n",
    "The default for max_depth is the depth necessary to form leaves that are pure. \n",
    "\n",
    "Let's try using some criteria for the decision trees to help increase its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79 (+/- 0.12)\n",
      "Accuracy: 0.80 (+/- 0.11)\n",
      "Accuracy: 0.79 (+/- 0.08)\n",
      "Accuracy: 0.81 (+/- 0.13)\n",
      "Accuracy: 0.80 (+/- 0.11)\n",
      "Accuracy: 0.80 (+/- 0.11)\n",
      "Accuracy: 0.80 (+/- 0.12)\n",
      "Accuracy: 0.80 (+/- 0.11)\n",
      "Accuracy: 0.79 (+/- 0.11)\n",
      "Accuracy: 0.82 (+/- 0.13)\n",
      "\n",
      "--- 0.2963099479675293 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Creating a for loop to run the model 10 times\n",
    "# Calculating the runtime for the 10 decision trees\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    # we will at first use default criteria for sklearns decision tree classifier \n",
    "    decision_tree = tree.DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_features=10,\n",
    "        max_depth=3,\n",
    "    )\n",
    "    \n",
    "    # We will examine the mean and 2*sigma value of the cross validation scores \n",
    "    # for each of the 10 iterations of model. \n",
    "    \n",
    "    score = cross_val_score(decision_tree, X, Y, cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By lowering the number of features to select from at each split and creating a tree with a depth of only 3, we have increased the accuracy of our decision tree! Using a max_depth of 2 or 4 lowers the accuracy. Three seems to be a great fit for our tree. It likely reduces overfitting. It seems a depth of 2 or less leads to underfitting and 4 or more leads to overfitting. \n",
    "\n",
    "While this is not as accurate as our decision tree, it is very close! It is also much less computationally expesive as apparent by the runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "While Random Forests are generally more accurate, averaging many Decision Trees trained on a subset of the data, they are more computationally expensive. The computer needs to create many Decision Trees and evaluate testing data by running these datapoints through each of these trees. \n",
    "\n",
    "On the other hand, Decision Trees are prone to overfitting the data. They branch based on the training data thus are a perfect fit for the training data. However, the slight differences in the training data and testing data can lead to misclassification of the testing data. \n",
    "\n",
    "Limiting a Decision Tree to a certain depth can increase the accuracy of the tree. It limits the models tendancy to overfit the training data. By stoping the tree at a certain depth, the leaves represent general trends in the training data rather than an exact description of said training data. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
