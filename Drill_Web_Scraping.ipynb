{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill: Introduction to Scrapping -- Kristofer Schobert\n",
    "\n",
    "In this notebook, we follow along with Thinkful's lessons on scrapping data from webpages. We are introduced to the package Scrapy as well as the use of APIs. \n",
    "\n",
    "This is very rough work. Most of it is uneditted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 11:36:45 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-07 11:36:45 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.2 (default, Dec 29 2018, 00:00:04) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.0.0-x86_64-i386-64bit\n",
      "2019-05-07 11:36:45 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-07 11:36:45 [scrapy.extensions.telnet] INFO: Telnet Password: d6a1f4a1259ee2ff\n",
      "2019-05-07 11:36:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-07 11:36:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-07 11:36:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-07 11:36:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-07 11:36:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-07 11:36:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-07 11:36:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-07 11:36:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://everydaysexism.com/> from <GET http://www.everydaysexism.com>\n",
      "2019-05-07 11:36:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://everydaysexism.com/> (referer: None)\n",
      "2019-05-07 11:36:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-07 11:36:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 438,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 12405,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 7, 18, 36, 46, 63116),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 9,\n",
      " 'memusage/max': 71409664,\n",
      " 'memusage/startup': 71409664,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2019, 5, 7, 18, 36, 45, 631951)}\n",
      "2019-05-07 11:36:46 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # What to do with the URL.  Here, we tell it to download all the code and save\n",
    "    # it to the mainpage.html file\n",
    "    def parse(self, response):\n",
    "        with open('mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Veronica</td>\n",
       "      <td>[Home]</td>\n",
       "      <td>[My brother says that there should be an All M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Zara</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[When I was fifteen I went skating with my sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Zara</td>\n",
       "      <td>[Public space, University]</td>\n",
       "      <td>[Two stories in different places, but they hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Veronika Didusenko, Miss Ukraine 2018</td>\n",
       "      <td>[Media, missuniverse, missworld, singlemoms, W...</td>\n",
       "      <td>[I am a professional model and have been walki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>[Friendships, Home, Public space, Workplace]</td>\n",
       "      <td>[I got called the mother of the business as on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                   name  \\\n",
       "0 2019-05-03                               Veronica   \n",
       "1 2019-05-03                                   Zara   \n",
       "2 2019-05-03                                   Zara   \n",
       "3 2019-05-03  Veronika Didusenko, Miss Ukraine 2018   \n",
       "4 2019-05-03                                  Kelly   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                             [Home]   \n",
       "1                                     [Public space]   \n",
       "2                         [Public space, University]   \n",
       "3  [Media, missuniverse, missworld, singlemoms, W...   \n",
       "4       [Friendships, Home, Public space, Workplace]   \n",
       "\n",
       "                                                text  \n",
       "0  [My brother says that there should be an All M...  \n",
       "1  [When I was fifteen I went skating with my sis...  \n",
       "2  [Two stories in different places, but they hap...  \n",
       "3  [I am a professional model and have been walki...  \n",
       "4  [I got called the mother of the business as on...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('firstpage.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "        # Get the URL of the previous page.\n",
    "        next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "        if next_page is not None and pagenum < 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'data.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Veronica</td>\n",
       "      <td>[Home]</td>\n",
       "      <td>[My brother says that there should be an All M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Zara</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[When I was fifteen I went skating with my sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Zara</td>\n",
       "      <td>[Public space, University]</td>\n",
       "      <td>[Two stories in different places, but they hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Veronika Didusenko, Miss Ukraine 2018</td>\n",
       "      <td>[Media, missuniverse, missworld, singlemoms, W...</td>\n",
       "      <td>[I am a professional model and have been walki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>[Friendships, Home, Public space, Workplace]</td>\n",
       "      <td>[I got called the mother of the business as on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Nora</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[There is a means of condescending to someone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>alice</td>\n",
       "      <td>[University]</td>\n",
       "      <td>[This guy told me his idea of dating advice th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Hannah</td>\n",
       "      <td>[Home, Public space, School, Stereotypes]</td>\n",
       "      <td>[For as long as I can remember I have broken e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Emily</td>\n",
       "      <td>[Friends social]</td>\n",
       "      <td>[I was 13 when it started this boy who one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Libby</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[On my first day of working at a new job, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Sophie</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[There are days when I am just going about my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Marta</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[I volunteer at a charity shop. Today, one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Nat</td>\n",
       "      <td>[Public space, Workplace]</td>\n",
       "      <td>[‘Stupid fucking woman’ – male colleague’s com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Nat</td>\n",
       "      <td>[Public space, Workplace]</td>\n",
       "      <td>[Smoking a roll-up, male colleague says ‘ooh –...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Spgs</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[Original tweet: “If you’re confident: Ugh wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Human Being</td>\n",
       "      <td>[male entitlement, misogyny, Sexism, Workplace]</td>\n",
       "      <td>[I lost my job for being an assertive female i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Janey</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[Got told off for being bossy in a restaurant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[Executive sexism, Workplace]</td>\n",
       "      <td>[I am an architectural consultant working on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[I am working on a major workplace project for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Sexism on the field</td>\n",
       "      <td>[School, University]</td>\n",
       "      <td>[Happens every Monday or any other PE lessons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Apoorva</td>\n",
       "      <td>[Home, Public space, School]</td>\n",
       "      <td>[I live in the orthodox and cultural country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>??</td>\n",
       "      <td>[Math, School, University]</td>\n",
       "      <td>[I’m a student at a small liberal arts college...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Mia C</td>\n",
       "      <td>[#4All, School]</td>\n",
       "      <td>[I’m running a campaign on the unfairness of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Anok</td>\n",
       "      <td>[Home]</td>\n",
       "      <td>[in African homes, sexism is so normal. my own...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[Pushed to the point of bulimia out of fear of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Pub Work</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[17, i shouldn’t be experiencing this kind of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Nobody special</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[A couple of years ago, some boys in my school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Angel</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[I was walking down the street in Colombia I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Lucia</td>\n",
       "      <td>[friends, Home, Public space, Scared]</td>\n",
       "      <td>[I was going with my friend down a street afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Nora</td>\n",
       "      <td>[Media]</td>\n",
       "      <td>[I recently learned about a horrendous case of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Tara</td>\n",
       "      <td>[University, woman]</td>\n",
       "      <td>[I remember a particular incident back in my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Nicole</td>\n",
       "      <td>[Home, Media, Public space, School, University]</td>\n",
       "      <td>[I am 22 years old, just finishing my universi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Emily</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[I was at work the other day when the assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Teen Workplace</td>\n",
       "      <td>[teenagers, Teens in work, Work, work place, W...</td>\n",
       "      <td>[As a male teenager it is incredibly difficult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Stine</td>\n",
       "      <td>[family, Home]</td>\n",
       "      <td>[I started wearing make up when I was 14 years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Christina</td>\n",
       "      <td>[facebook, Media]</td>\n",
       "      <td>[Just scrolled through facebook to come across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>KT</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[I was a freshman at the time. My school has t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>KT</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[I was a sophomore in high school. I was in st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>KT</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[I was a sophomore at the time. This is anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Dana</td>\n",
       "      <td>[Home, Public space, School, Workplace]</td>\n",
       "      <td>[I’ve been viewed as a sexual “thing” my whole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>“Hello gorgeous”</td>\n",
       "      <td>[improper behaviour, Journalism, Uncomfortable...</td>\n",
       "      <td>[I am a woman in my early thirties and I work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Erin</td>\n",
       "      <td>[Media, Public space]</td>\n",
       "      <td>[I saw a youtube video by Vice called somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>name</td>\n",
       "      <td>[Media, Workplace]</td>\n",
       "      <td>[I saw someone with a “killing raping sleeping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Claire</td>\n",
       "      <td>[Home]</td>\n",
       "      <td>[Gotta cover up (long pants, long sleeves, loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Boys Club</td>\n",
       "      <td>[Career, pay gap, promotions, Workplace]</td>\n",
       "      <td>[There were 3 rounds of lay offs at the office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>[friends]</td>\n",
       "      <td>[First, I have to tell, that most of my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Payton</td>\n",
       "      <td>[Home, School]</td>\n",
       "      <td>[I have had to deal with sexism on a daily bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Burnt out teacher</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[I’m a music teacher, I give individual lesson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>[# Video games # memes, Media]</td>\n",
       "      <td>[I saw a  video game meme with two women with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Kylie</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I’m normally a recluse, and i often stay at h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Summer</td>\n",
       "      <td>[Home, Public space, University]</td>\n",
       "      <td>[I’m at uni and I live in dorms, all my friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Madi</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I am 20 years old and i trust men, women, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Chris</td>\n",
       "      <td>[clothes, Public space, Shopping, Shops]</td>\n",
       "      <td>[Why when I go shopping for clothes do I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>CC</td>\n",
       "      <td>[health, hypocrisy, Life, man flu, Public spac...</td>\n",
       "      <td>[Man flu when men come down with an ailment th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Maddie</td>\n",
       "      <td>[body image, Home, weight]</td>\n",
       "      <td>[My big sister is my best friend. She gained s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Meadow Conner</td>\n",
       "      <td>[Public space]</td>\n",
       "      <td>[On my way home from school, I was walking acr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>person</td>\n",
       "      <td>[Workplace]</td>\n",
       "      <td>[during the world cup last year all the male m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>Maddie</td>\n",
       "      <td>[Home, Public Transport]</td>\n",
       "      <td>[I’m not allowed to catch buses on my own at 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>person</td>\n",
       "      <td>[Public space, School]</td>\n",
       "      <td>[I go to an all girl’s school so sexism isn’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2019-04-13</td>\n",
       "      <td>person</td>\n",
       "      <td>[School]</td>\n",
       "      <td>[at the local boys school they don’t even have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                   name  \\\n",
       "0  2019-05-03                               Veronica   \n",
       "1  2019-05-03                                   Zara   \n",
       "2  2019-05-03                                   Zara   \n",
       "3  2019-05-03  Veronika Didusenko, Miss Ukraine 2018   \n",
       "4  2019-05-03                                  Kelly   \n",
       "5  2019-05-03                                   Nora   \n",
       "6  2019-05-03                                  alice   \n",
       "7  2019-05-03                                 Hannah   \n",
       "8  2019-05-03                                  Emily   \n",
       "9  2019-05-03                                  Libby   \n",
       "10 2019-05-03                                 Sophie   \n",
       "11 2019-05-03                                  Marta   \n",
       "12 2019-05-01                                    Nat   \n",
       "13 2019-05-01                                    Nat   \n",
       "14 2019-05-01                                   Spgs   \n",
       "15 2019-05-01                            Human Being   \n",
       "16 2019-05-01                                  Janey   \n",
       "17 2019-05-01                              Charlotte   \n",
       "18 2019-05-01                              Charlotte   \n",
       "19 2019-05-01                    Sexism on the field   \n",
       "20 2019-05-01                                Apoorva   \n",
       "21 2019-05-01                                     ??   \n",
       "22 2019-05-01                                  Mia C   \n",
       "23 2019-05-01                                   Anok   \n",
       "24 2019-05-01                              Anonymous   \n",
       "25 2019-05-01                               Pub Work   \n",
       "26 2019-05-01                         Nobody special   \n",
       "27 2019-05-01                                  Angel   \n",
       "28 2019-05-01                                  Lucia   \n",
       "29 2019-05-01                                   Nora   \n",
       "..        ...                                    ...   \n",
       "60 2019-04-27                                   Tara   \n",
       "61 2019-04-27                                 Nicole   \n",
       "62 2019-04-27                                  Emily   \n",
       "63 2019-04-27                         Teen Workplace   \n",
       "64 2019-04-27                                  Stine   \n",
       "65 2019-04-27                              Christina   \n",
       "66 2019-04-27                                     KT   \n",
       "67 2019-04-27                                     KT   \n",
       "68 2019-04-27                                     KT   \n",
       "69 2019-04-27                                   Dana   \n",
       "70 2019-04-27                       “Hello gorgeous”   \n",
       "71 2019-04-27                                   Erin   \n",
       "72 2019-04-27                                   name   \n",
       "73 2019-04-27                                 Claire   \n",
       "74 2019-04-27                              Boys Club   \n",
       "75 2019-04-27                                  Kathy   \n",
       "76 2019-04-13                                 Payton   \n",
       "77 2019-04-13                      Burnt out teacher   \n",
       "78 2019-04-13                              anonymous   \n",
       "79 2019-04-13                                  Kylie   \n",
       "80 2019-04-13                                 Summer   \n",
       "81 2019-04-13                                   Madi   \n",
       "82 2019-04-13                                  Chris   \n",
       "83 2019-04-13                                     CC   \n",
       "84 2019-04-13                                 Maddie   \n",
       "85 2019-04-13                          Meadow Conner   \n",
       "86 2019-04-13                                 person   \n",
       "87 2019-04-13                                 Maddie   \n",
       "88 2019-04-13                                 person   \n",
       "89 2019-04-13                                 person   \n",
       "\n",
       "                                                 tags  \\\n",
       "0                                              [Home]   \n",
       "1                                      [Public space]   \n",
       "2                          [Public space, University]   \n",
       "3   [Media, missuniverse, missworld, singlemoms, W...   \n",
       "4        [Friendships, Home, Public space, Workplace]   \n",
       "5                                      [Public space]   \n",
       "6                                        [University]   \n",
       "7           [Home, Public space, School, Stereotypes]   \n",
       "8                                    [Friends social]   \n",
       "9                                         [Workplace]   \n",
       "10                                     [Public space]   \n",
       "11                                        [Workplace]   \n",
       "12                          [Public space, Workplace]   \n",
       "13                          [Public space, Workplace]   \n",
       "14                                     [Public space]   \n",
       "15    [male entitlement, misogyny, Sexism, Workplace]   \n",
       "16                                     [Public space]   \n",
       "17                      [Executive sexism, Workplace]   \n",
       "18                                        [Workplace]   \n",
       "19                               [School, University]   \n",
       "20                       [Home, Public space, School]   \n",
       "21                         [Math, School, University]   \n",
       "22                                    [#4All, School]   \n",
       "23                                             [Home]   \n",
       "24                                           [School]   \n",
       "25                                        [Workplace]   \n",
       "26                                           [School]   \n",
       "27                                     [Public space]   \n",
       "28              [friends, Home, Public space, Scared]   \n",
       "29                                            [Media]   \n",
       "..                                                ...   \n",
       "60                                [University, woman]   \n",
       "61    [Home, Media, Public space, School, University]   \n",
       "62                                        [Workplace]   \n",
       "63  [teenagers, Teens in work, Work, work place, W...   \n",
       "64                                     [family, Home]   \n",
       "65                                  [facebook, Media]   \n",
       "66                                           [School]   \n",
       "67                                           [School]   \n",
       "68                                           [School]   \n",
       "69            [Home, Public space, School, Workplace]   \n",
       "70  [improper behaviour, Journalism, Uncomfortable...   \n",
       "71                              [Media, Public space]   \n",
       "72                                 [Media, Workplace]   \n",
       "73                                             [Home]   \n",
       "74           [Career, pay gap, promotions, Workplace]   \n",
       "75                                          [friends]   \n",
       "76                                     [Home, School]   \n",
       "77                                           [School]   \n",
       "78                     [# Video games # memes, Media]   \n",
       "79                                                 []   \n",
       "80                   [Home, Public space, University]   \n",
       "81                                                 []   \n",
       "82           [clothes, Public space, Shopping, Shops]   \n",
       "83  [health, hypocrisy, Life, man flu, Public spac...   \n",
       "84                         [body image, Home, weight]   \n",
       "85                                     [Public space]   \n",
       "86                                        [Workplace]   \n",
       "87                           [Home, Public Transport]   \n",
       "88                             [Public space, School]   \n",
       "89                                           [School]   \n",
       "\n",
       "                                                 text  \n",
       "0   [My brother says that there should be an All M...  \n",
       "1   [When I was fifteen I went skating with my sis...  \n",
       "2   [Two stories in different places, but they hap...  \n",
       "3   [I am a professional model and have been walki...  \n",
       "4   [I got called the mother of the business as on...  \n",
       "5   [There is a means of condescending to someone ...  \n",
       "6   [This guy told me his idea of dating advice th...  \n",
       "7   [For as long as I can remember I have broken e...  \n",
       "8   [I was 13 when it started this boy who one of ...  \n",
       "9   [On my first day of working at a new job, I wa...  \n",
       "10  [There are days when I am just going about my ...  \n",
       "11  [I volunteer at a charity shop. Today, one of ...  \n",
       "12  [‘Stupid fucking woman’ – male colleague’s com...  \n",
       "13  [Smoking a roll-up, male colleague says ‘ooh –...  \n",
       "14  [Original tweet: “If you’re confident: Ugh wha...  \n",
       "15  [I lost my job for being an assertive female i...  \n",
       "16  [Got told off for being bossy in a restaurant ...  \n",
       "17  [I am an architectural consultant working on a...  \n",
       "18  [I am working on a major workplace project for...  \n",
       "19  [Happens every Monday or any other PE lessons ...  \n",
       "20  [I live in the orthodox and cultural country o...  \n",
       "21  [I’m a student at a small liberal arts college...  \n",
       "22  [I’m running a campaign on the unfairness of t...  \n",
       "23  [in African homes, sexism is so normal. my own...  \n",
       "24  [Pushed to the point of bulimia out of fear of...  \n",
       "25  [17, i shouldn’t be experiencing this kind of ...  \n",
       "26  [A couple of years ago, some boys in my school...  \n",
       "27  [I was walking down the street in Colombia I a...  \n",
       "28  [I was going with my friend down a street afte...  \n",
       "29  [I recently learned about a horrendous case of...  \n",
       "..                                                ...  \n",
       "60  [I remember a particular incident back in my f...  \n",
       "61  [I am 22 years old, just finishing my universi...  \n",
       "62  [I was at work the other day when the assistan...  \n",
       "63  [As a male teenager it is incredibly difficult...  \n",
       "64  [I started wearing make up when I was 14 years...  \n",
       "65  [Just scrolled through facebook to come across...  \n",
       "66  [I was a freshman at the time. My school has t...  \n",
       "67  [I was a sophomore in high school. I was in st...  \n",
       "68  [I was a sophomore at the time. This is anothe...  \n",
       "69  [I’ve been viewed as a sexual “thing” my whole...  \n",
       "70  [I am a woman in my early thirties and I work ...  \n",
       "71  [I saw a youtube video by Vice called somethin...  \n",
       "72  [I saw someone with a “killing raping sleeping...  \n",
       "73  [Gotta cover up (long pants, long sleeves, loo...  \n",
       "74  [There were 3 rounds of lay offs at the office...  \n",
       "75  [First, I have to tell, that most of my friend...  \n",
       "76  [I have had to deal with sexism on a daily bas...  \n",
       "77  [I’m a music teacher, I give individual lesson...  \n",
       "78  [I saw a  video game meme with two women with ...  \n",
       "79  [I’m normally a recluse, and i often stay at h...  \n",
       "80  [I’m at uni and I live in dorms, all my friend...  \n",
       "81  [I am 20 years old and i trust men, women, chi...  \n",
       "82  [Why when I go shopping for clothes do I have ...  \n",
       "83  [Man flu when men come down with an ailment th...  \n",
       "84  [My big sister is my best friend. She gained s...  \n",
       "85  [On my way home from school, I was walking acr...  \n",
       "86  [during the world cup last year all the male m...  \n",
       "87  [I’m not allowed to catch buses on my own at 1...  \n",
       "88  [I go to an all girl’s school so sexism isn’t ...  \n",
       "89  [at the local boys school they don’t even have...  \n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data from all 9 pages\n",
    "ESSdf=pd.read_json('data.json', orient='records')\n",
    "print(ESSdf.shape)\n",
    "ESSdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beachcomber (pen name)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bing Crosby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob Costas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comedy film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List of comedians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Bowie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do Not Adjust Your Set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elvis Presley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ennio Morricone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cinema of the United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fawlty Towers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Federico Fellini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>George Harrison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gilbert and Sullivan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Graham Chapman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hannibal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Harry Kroto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ingmar Bergman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>John Stuart Mill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>John Cleese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>J. K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>John Peel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Kate Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Monty Python's Life of Brian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Spam (Monty Python)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Dead Parrot sketch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cheese Shop sketch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bruces' Philosophers Song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The Funniest Joke in the World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Spanish Inquisition (Monty Python)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Michael Palin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title\n",
       "0                   Beachcomber (pen name)\n",
       "1                              Bing Crosby\n",
       "2                               Bob Costas\n",
       "3                              Comedy film\n",
       "4                        List of comedians\n",
       "5                            Douglas Adams\n",
       "6                              David Bowie\n",
       "7                   Do Not Adjust Your Set\n",
       "8                            Elvis Presley\n",
       "9                          Ennio Morricone\n",
       "10            Cinema of the United Kingdom\n",
       "11                           Fawlty Towers\n",
       "12                        Federico Fellini\n",
       "13                         George Harrison\n",
       "14                    Gilbert and Sullivan\n",
       "15                          Graham Chapman\n",
       "16                                Hannibal\n",
       "17                      Hunter S. Thompson\n",
       "18                             Harry Kroto\n",
       "19                          Ingmar Bergman\n",
       "20                        John Stuart Mill\n",
       "21                             John Cleese\n",
       "22                           J. K. Rowling\n",
       "23                               John Peel\n",
       "24                                    Joke\n",
       "25                               Kate Bush\n",
       "26            Monty Python's Life of Brian\n",
       "27                     Spam (Monty Python)\n",
       "28                      Dead Parrot sketch\n",
       "29                      Cheese Shop sketch\n",
       "30               Bruces' Philosophers Song\n",
       "31          The Funniest Joke in the World\n",
       "32  The Spanish Inquisition (Monty Python)\n",
       "33                           Michael Palin"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "Monty=pd.read_json('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/PythonLinks.json', orient='records')\n",
    "print(Monty.shape)\n",
    "Monty.head(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering data on day length times from sunrise-sunset.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 23:29:03 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-09 23:29:03 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.2 (default, Dec 29 2018, 00:00:04) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.0.0-x86_64-i386-64bit\n",
      "2019-05-09 23:29:03 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'sunrise_sunset.json'}\n",
      "2019-05-09 23:29:03 [scrapy.extensions.telnet] INFO: Telnet Password: e78bd46e8b36f718\n",
      "2019-05-09 23:29:03 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-09 23:29:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-09 23:29:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-09 23:29:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-09 23:29:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-09 23:29:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-09 23:29:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-09 23:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=0.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=0.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '6:10:30 AM', 'sunset': '6:17:39 PM', 'solar_noon': '12:14:04 PM', 'day_length': '12:07:09', 'civil_twilight_begin': '5:48:51 AM', 'civil_twilight_end': '6:39:18 PM', 'nautical_twilight_begin': '5:23:37 AM', 'nautical_twilight_end': '7:04:32 PM', 'astronomical_twilight_begin': '4:58:20 AM', 'astronomical_twilight_end': '7:29:49 PM'}\n",
      "2019-05-09 23:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=10.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=10.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '5:57:30 AM', 'sunset': '6:30:39 PM', 'solar_noon': '12:14:04 PM', 'day_length': '12:33:09', 'civil_twilight_begin': '5:35:24 AM', 'civil_twilight_end': '6:52:45 PM', 'nautical_twilight_begin': '5:09:28 AM', 'nautical_twilight_end': '7:18:41 PM', 'astronomical_twilight_begin': '4:43:15 AM', 'astronomical_twilight_end': '7:44:54 PM'}\n",
      "2019-05-09 23:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=20.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=20.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '5:43:28 AM', 'sunset': '6:44:41 PM', 'solar_noon': '12:14:04 PM', 'day_length': '13:01:13', 'civil_twilight_begin': '5:20:05 AM', 'civil_twilight_end': '7:08:04 PM', 'nautical_twilight_begin': '4:52:23 AM', 'nautical_twilight_end': '7:35:46 PM', 'astronomical_twilight_begin': '4:24:02 AM', 'astronomical_twilight_end': '8:04:07 PM'}\n",
      "2019-05-09 23:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=50.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=50.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '4:38:17 AM', 'sunset': '7:49:52 PM', 'solar_noon': '12:14:04 PM', 'day_length': '15:11:35', 'civil_twilight_begin': '3:59:55 AM', 'civil_twilight_end': '8:28:14 PM', 'nautical_twilight_begin': '3:09:23 AM', 'nautical_twilight_end': '9:18:46 PM', 'astronomical_twilight_begin': '2:04:16 AM', 'astronomical_twilight_end': '10:23:52 PM'}\n",
      "2019-05-09 23:29:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=40.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=40.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '5:06:52 AM', 'sunset': '7:21:17 PM', 'solar_noon': '12:14:04 PM', 'day_length': '14:14:25', 'civil_twilight_begin': '4:36:44 AM', 'civil_twilight_end': '7:51:25 PM', 'nautical_twilight_begin': '3:59:34 AM', 'nautical_twilight_end': '8:28:35 PM', 'astronomical_twilight_begin': '3:18:48 AM', 'astronomical_twilight_end': '9:09:21 PM'}\n",
      "2019-05-09 23:29:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=30.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=30.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '5:27:15 AM', 'sunset': '7:00:54 PM', 'solar_noon': '12:14:04 PM', 'day_length': '13:33:39', 'civil_twilight_begin': '5:01:26 AM', 'civil_twilight_end': '7:26:43 PM', 'nautical_twilight_begin': '4:30:25 AM', 'nautical_twilight_end': '7:57:44 PM', 'astronomical_twilight_begin': '3:57:57 AM', 'astronomical_twilight_end': '8:30:12 PM'}\n",
      "2019-05-09 23:29:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=60.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=60.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '3:50:37 AM', 'sunset': '8:37:32 PM', 'solar_noon': '12:14:04 PM', 'day_length': '16:46:55', 'civil_twilight_begin': '2:51:05 AM', 'civil_twilight_end': '9:37:04 PM', 'nautical_twilight_begin': '12:43:53 AM', 'nautical_twilight_end': '11:44:16 PM', 'astronomical_twilight_begin': '12:00:01 AM', 'astronomical_twilight_end': '12:00:01 AM'}\n",
      "2019-05-09 23:29:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.sunrise-sunset.org/json?lat=70.01&lng=-4.4203400%3E> (referer: None)\n",
      "2019-05-09 23:29:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://api.sunrise-sunset.org/json?lat=70.01&lng=-4.4203400%3E>\n",
      "{'sunrise': '1:43:02 AM', 'sunset': '10:45:07 PM', 'solar_noon': '12:14:04 PM', 'day_length': '21:02:05', 'civil_twilight_begin': '12:00:01 AM', 'civil_twilight_end': '12:00:01 AM', 'nautical_twilight_begin': '12:00:01 AM', 'nautical_twilight_end': '12:00:01 AM', 'astronomical_twilight_begin': '12:00:01 AM', 'astronomical_twilight_end': '12:00:01 AM'}\n",
      "2019-05-09 23:29:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-09 23:29:05 [scrapy.extensions.feedexport] INFO: Stored json feed (8 items) in: sunrise_sunset.json\n",
      "2019-05-09 23:29:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2023,\n",
      " 'downloader/request_count': 8,\n",
      " 'downloader/request_method_count/GET': 8,\n",
      " 'downloader/response_bytes': 3074,\n",
      " 'downloader/response_count': 8,\n",
      " 'downloader/response_status_count/200': 8,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 10, 6, 29, 5, 845545),\n",
      " 'item_scraped_count': 8,\n",
      " 'log_count/DEBUG': 16,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 71925760,\n",
      " 'memusage/startup': 71921664,\n",
      " 'response_received_count': 8,\n",
      " 'scheduler/dequeued': 8,\n",
      " 'scheduler/dequeued/memory': 8,\n",
      " 'scheduler/enqueued': 8,\n",
      " 'scheduler/enqueued/memory': 8,\n",
      " 'start_time': datetime.datetime(2019, 5, 10, 6, 29, 4, 137328)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 23:29:05 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import json\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "class SunSpider(scrapy.Spider):\n",
    "    name = \"SS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = ['https://api.sunrise-sunset.org/json?lat='\n",
    "                      + str(i + .01) + '&lng=-4.4203400>' for i in range(0,80,10)]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # the files are returned in json format\n",
    "        jsonresponse = json.loads(response.body_as_unicode())\n",
    "\n",
    "        return jsonresponse['results']\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'sunrise_sunset.json'\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(SunSpider)\n",
    "process.start()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy lists data from webpages in the order of which webpage completed downloading first. \n",
    "# this is not the order start_urls page.\n",
    "\n",
    "latitude_in_download_order = [0.01, 10.01, 20.01, 50.01, 40.01, 30.01, 60.01, 70.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 23:36:32 [matplotlib] DEBUG: $HOME=/Users/Kris\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: CONFIGDIR=/Users/Kris/.matplotlib\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: matplotlib data path: /anaconda3/envs/first_sandbox/lib/python3.7/site-packages/matplotlib/mpl-data\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: loaded rc file /anaconda3/envs/first_sandbox/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: matplotlib version 3.0.3\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: interactive is False\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: platform is darwin\n",
      "2019-05-09 23:36:32 [matplotlib] DEBUG: loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', '_bootlocale', '_locale', 'site', 'os', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'mpl_toolkits', 'zope', 'runpy', 'pkgutil', 'weakref', '_weakrefset', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', 'errno', '_posixsubprocess', 'select', 'selectors', 'collections.abc', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'gettext', 'ast', '_ast', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'textwrap', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'socket', '_socket', 'ssl', '_ssl', 'base64', 'asyncio.constants', 'asyncio.coroutines', 'asyncio.base_futures', 'asyncio.format_helpers', 'asyncio.log', 'asyncio.events', 'contextvars', '_contextvars', 'asyncio.base_tasks', '_asyncio', 'asyncio.futures', 'asyncio.protocols', 'asyncio.sslproto', 'asyncio.transports', 'asyncio.tasks', 'asyncio.locks', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'pickleshare', 'pickle', '_compat_pickle', '_pickle', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'six.moves', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.async_generator', 'queue', '_queue', 'six.moves.queue', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters.cli', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.document', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.vt100', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'array', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'jedi._compatibility', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.compiled.fake', 'jedi.evaluate.analysis', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.context.function', 'jedi.evaluate.docstrings', 'jedi.evaluate.pep0484', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.context.asynchronous', 'jedi.evaluate.parser_cache', 'jedi.evaluate.context.instance', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', '_scproxy', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_5', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', '_uuid', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'ipywidgets', 'ipywidgets._version', 'ipywidgets.widgets', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'scrapy', 'scrapy._monkeypatches', 'twisted', 'twisted._version', 'incremental', 'incremental._version', 'twisted.persisted', 'twisted.persisted.styles', 'twisted.python', 'twisted.python.compat', 'http.cookiejar', 'twisted.python.deprecate', 'twisted.python.versions', 'twisted.python.log', 'zope.interface', 'zope.interface.interface', 'zope.interface.exceptions', 'zope.interface.ro', 'zope.interface._zope_interface_coptimizations', 'zope.interface.declarations', 'zope.interface.advice', 'zope.interface._compat', 'zope.interface.interfaces', 'twisted.python.context', 'twisted.python._oldstyle', 'twisted.python.reflect', 'twisted.python.util', 'twisted.python.failure', 'twisted.python.threadable', 'twisted.logger', 'twisted.logger._levels', 'constantly', 'constantly._constants', 'constantly._version', 'twisted.logger._flatten', 'twisted.logger._format', 'twisted.python._tzhelper', 'twisted.logger._logger', 'twisted.logger._global', 'twisted.logger._buffer', 'twisted.logger._observer', 'twisted.logger._filter', 'twisted.logger._io', 'twisted.logger._file', 'twisted.logger._stdlib', 'twisted.logger._legacy', 'twisted.logger._json', 'scrapy.spiders', 'scrapy.signals', 'scrapy.http', 'scrapy.http.headers', 'w3lib', 'w3lib.http', 'scrapy.utils', 'scrapy.utils.datatypes', 'scrapy.exceptions', 'scrapy.utils.python', 'scrapy.utils.decorators', 'twisted.internet', 'twisted.internet.defer', 'attr', 'attr.converters', 'attr._make', 'attr._config', 'attr._compat', 'attr.exceptions', 'attr.filters', 'attr.validators', 'attr._funcs', 'twisted.python.lockfile', 'twisted.python.runtime', 'twisted.internet.threads', 'scrapy.http.request', 'w3lib.url', 'six.moves.urllib', 'six.moves.urllib.parse', 'six.moves.urllib.request', 'w3lib.util', 'scrapy.utils.trackref', 'scrapy.utils.url', 'scrapy.http.common', 'scrapy.http.request.form', 'lxml', 'lxml.html', 'lxml.etree', '_cython_0_29_6', 'lxml._elementpath', 'gzip', 'lxml.html.defs', 'lxml.html._setmixin', 'parsel', 'parsel.selector', 'parsel.utils', 'w3lib.html', 'parsel.csstranslator', 'cssselect', 'cssselect.parser', 'cssselect.xpath', 'parsel.xpathfuncs', 'scrapy.utils.response', 'webbrowser', 'twisted.web', 'twisted.web.http', 'cgi', 'twisted.python.components', 'zope.interface.adapter', 'twisted.internet.interfaces', 'twisted.internet.protocol', 'twisted.internet.error', 'twisted.internet.address', 'twisted.python.filepath', 'twisted.python.win32', 'twisted.internet._producer_helpers', 'twisted.internet.task', 'twisted.internet.base', 'twisted.internet.fdesc', 'twisted.internet.main', 'twisted.internet.abstract', 'twisted.internet._resolver', 'twisted.internet._idna', 'twisted.protocols', 'twisted.protocols.policies', 'twisted.protocols.basic', 'twisted.web.iweb', 'twisted.cred', 'twisted.cred.credentials', 'twisted.python.randbytes', 'twisted.cred._digest', 'twisted.cred.error', 'twisted.web.http_headers', 'twisted.web._responses', 'scrapy.http.request.rpc', 'xmlrpc', 'xmlrpc.client', 'xml.parsers', 'xml.parsers.expat', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'scrapy.http.response', 'scrapy.link', 'scrapy.http.response.html', 'scrapy.http.response.text', 'w3lib.encoding', 'scrapy.http.response.xml', 'scrapy.utils.deprecate', 'scrapy.spiders.crawl', 'scrapy.utils.spider', 'scrapy.utils.misc', 'scrapy.item', 'scrapy.spiders.feed', 'scrapy.utils.iterators', 'csv', '_csv', 'scrapy.selector', 'scrapy.selector.unified', 'scrapy.selector.lxmlsel', 'scrapy.spiders.sitemap', 'scrapy.utils.sitemap', 'scrapy.utils.gz', 'scrapy.crawler', 'twisted.internet.default', 'twisted.internet.selectreactor', 'twisted.internet.posixbase', 'twisted.internet.udp', 'twisted.internet.tcp', 'twisted.internet._newtls', 'twisted.protocols.tls', 'OpenSSL', 'OpenSSL.crypto', 'cryptography', 'cryptography.__about__', 'cryptography.x509', 'cryptography.x509.certificate_transparency', 'cryptography.x509.base', 'cryptography.utils', 'cryptography.hazmat', 'cryptography.hazmat.primitives', 'cryptography.hazmat.primitives.asymmetric', 'cryptography.hazmat.primitives.asymmetric.dsa', 'cryptography.hazmat.primitives.asymmetric.ec', 'cryptography.hazmat._oid', 'cryptography.hazmat.primitives.asymmetric.rsa', 'cryptography.exceptions', 'cryptography.hazmat.backends', 'cryptography.hazmat.backends.interfaces', 'cryptography.x509.extensions', 'ipaddress', 'asn1crypto', 'asn1crypto.version', 'asn1crypto.keys', 'asn1crypto._elliptic_curve', 'asn1crypto._int', 'asn1crypto.util', 'asn1crypto._errors', 'asn1crypto._iri', 'encodings.idna', 'stringprep', 'asn1crypto._types', 'asn1crypto._ordereddict', 'asn1crypto._ffi', 'asn1crypto._perf', 'asn1crypto._perf._big_num_ctypes', 'ctypes.util', 'ctypes.macholib', 'ctypes.macholib.dyld', 'ctypes.macholib.framework', 'ctypes.macholib.dylib', 'asn1crypto.algos', 'asn1crypto.core', 'asn1crypto._teletex_codec', 'asn1crypto.parser', 'cryptography.hazmat.primitives.constant_time', 'cryptography.hazmat.bindings', '_cffi_backend', '_constant_time.lib', '_constant_time', 'cryptography.hazmat.bindings._constant_time', 'cryptography.hazmat.primitives.serialization', 'cryptography.hazmat.primitives.serialization.base', 'cryptography.hazmat.primitives.serialization.ssh', 'cryptography.hazmat.primitives.asymmetric.ed25519', 'cryptography.x509.general_name', 'cryptography.x509.name', 'cryptography.x509.oid', 'cryptography.hazmat.primitives.hashes', 'OpenSSL._util', 'cryptography.hazmat.bindings.openssl', 'cryptography.hazmat.bindings.openssl.binding', '_openssl.lib', '_openssl', 'cryptography.hazmat.bindings._openssl', 'cryptography.hazmat.bindings.openssl._conditional', 'OpenSSL.SSL', 'OpenSSL.version', 'twisted.internet._sslverify', 'service_identity', 'service_identity.cryptography', 'pyasn1', 'pyasn1.codec', 'pyasn1.codec.der', 'pyasn1.codec.der.decoder', 'pyasn1.codec.cer', 'pyasn1.codec.cer.decoder', 'pyasn1.error', 'pyasn1.codec.ber', 'pyasn1.codec.ber.decoder', 'pyasn1.debug', 'pyasn1.compat', 'pyasn1.compat.octets', 'pyasn1.codec.ber.eoo', 'pyasn1.type', 'pyasn1.type.base', 'pyasn1.compat.calling', 'pyasn1.type.constraint', 'pyasn1.type.error', 'pyasn1.type.tag', 'pyasn1.type.tagmap', 'pyasn1.compat.integer', 'pyasn1.type.char', 'pyasn1.type.univ', 'pyasn1.compat.binary', 'pyasn1.type.namedtype', 'pyasn1.type.namedval', 'pyasn1.type.useful', 'pyasn1.compat.dateandtime', 'pyasn1.compat.string', 'service_identity._common', 'service_identity._compat', 'service_identity.exceptions', 'idna', 'idna.package_data', 'idna.core', 'idna.idnadata', 'idna.intranges', 'service_identity.pyopenssl', 'pyasn1_modules', 'pyasn1_modules.rfc2459', 'pyasn1.type.opentype', 'twisted.internet.unix', 'twisted.python.sendmsg', 'twisted.internet.process', 'twisted.internet._baseprocess', 'twisted.internet._signals', 'twisted.internet.reactor', 'zope.interface.verify', 'scrapy.core', 'scrapy.core.engine', 'scrapy.core.scraper', 'scrapy.utils.defer', 'scrapy.utils.log', 'logging.config', 'socketserver', 'scrapy.settings', 'scrapy.settings.default_settings', 'scrapy.utils.versions', 'scrapy.core.spidermw', 'scrapy.middleware', 'scrapy.utils.conf', 'configparser', 'six.moves.configparser', 'scrapy.utils.request', 'scrapy.utils.httpobj', 'scrapy.utils.reactor', 'scrapy.resolver', 'scrapy.interfaces', 'scrapy.extension', 'scrapy.signalmanager', 'pydispatch', 'pydispatch.dispatcher', 'pydispatch.saferef', 'pydispatch.robustapply', 'pydispatch.errors', 'scrapy.utils.signal', 'scrapy.utils.ossignal', 'scrapy.loader', 'scrapy.loader.common', 'scrapy.loader.processors', 'scrapy.spiderloader', 'scrapy.statscollectors', 'scrapy.logformatter', 'scrapy.extensions', 'scrapy.extensions.corestats', 'scrapy.extensions.telnet', 'twisted.conch', 'twisted.conch.manhole', 'twisted.conch.recvline', 'twisted.conch.insults', 'twisted.conch.insults.insults', 'twisted.conch.insults.helper', 'twisted.python._textattributes', 'twisted.python.htmlizer', 'twisted.conch.telnet', 'scrapy.utils.engine', 'scrapy.extensions.memusage', 'scrapy.mail', 'email.mime', 'email.mime.multipart', 'email.mime.base', 'email.policy', 'email.headerregistry', 'email._header_value_parser', 'email.contentmanager', 'six.moves.email_mime_multipart', 'email.mime.text', 'email.mime.nonmultipart', 'six.moves.email_mime_text', 'six.moves.email_mime_base', 'twisted.internet.ssl', 'scrapy.extensions.memdebug', 'scrapy.extensions.closespider', 'scrapy.extensions.feedexport', 'ftplib', 'scrapy.utils.ftp', 'scrapy.utils.boto', 'scrapy.exporters', 'xml.sax', 'xml.sax.xmlreader', 'xml.sax.handler', 'xml.sax._exceptions', 'xml.sax.saxutils', 'scrapy.utils.serialize', 'scrapy.extensions.logstats', 'scrapy.extensions.spiderstate', 'scrapy.utils.job', 'scrapy.extensions.throttle', 'scrapy.core.scheduler', 'scrapy.utils.reqser', 'scrapy.core.downloader', 'scrapy.core.downloader.middleware', 'scrapy.core.downloader.handlers', 'scrapy.core.downloader.handlers.datauri', 'scrapy.responsetypes', 'scrapy.core.downloader.handlers.file', 'scrapy.core.downloader.handlers.http', 'scrapy.core.downloader.handlers.http10', 'scrapy.core.downloader.handlers.http11', 'twisted.web.client', 'twisted.internet.endpoints', 'twisted.internet.stdio', 'twisted.internet._posixstdio', 'twisted.plugin', 'twisted.python.modules', 'twisted.python.zippath', 'zipfile', 'twisted.python.systemd', 'twisted.web.error', 'twisted.web._newclient', 'scrapy.core.downloader.webclient', 'scrapy.core.downloader.tls', 'scrapy.core.downloader.contextfactory', 'scrapy.core.downloader.handlers.s3', 'scrapy.core.downloader.handlers.ftp', 'twisted.protocols.ftp', 'twisted.copyright', 'twisted.cred.portal', 'twisted.cred.checkers', 'scrapy.downloadermiddlewares', 'scrapy.downloadermiddlewares.robotstxt', 'scrapy.downloadermiddlewares.httpauth', 'scrapy.downloadermiddlewares.downloadtimeout', 'scrapy.downloadermiddlewares.defaultheaders', 'scrapy.downloadermiddlewares.useragent', 'scrapy.downloadermiddlewares.retry', 'scrapy.downloadermiddlewares.ajaxcrawl', 'scrapy.downloadermiddlewares.redirect', 'scrapy.downloadermiddlewares.httpcompression', 'scrapy.downloadermiddlewares.cookies', 'scrapy.http.cookies', 'six.moves.http_cookiejar', 'scrapy.downloadermiddlewares.httpproxy', 'scrapy.downloadermiddlewares.stats', 'scrapy.downloadermiddlewares.httpcache', 'scrapy.spidermiddlewares', 'scrapy.spidermiddlewares.httperror', 'scrapy.spidermiddlewares.offsite', 'scrapy.spidermiddlewares.referer', 'scrapy.spidermiddlewares.urllength', 'scrapy.spidermiddlewares.depth', 'scrapy.pipelines', 'scrapy.dupefilters', 'queuelib', 'queuelib.queue', 'queuelib.pqueue', 'queuelib.rrqueue', 'scrapy.squeues', 'twisted.python.threadpool', 'twisted._threads', 'twisted._threads._threadworker', 'twisted._threads._ithreads', 'twisted._threads._convenience', 'twisted._threads._team', 'twisted._threads._memory', 'twisted._threads._pool', 'encodings.charmap', 'encodings.cp1252', 'pandas', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy._mklinit', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._internal', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.utils', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'mkl_fft', 'mkl_fft._pydfti', '_cython_0_29_2', 'mkl_fft._version', 'mkl_fft._numpy_fft', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.mtrand', 'mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'pandas.compat', 'distutils', 'distutils.version', 'pandas.compat.chainmap', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.tslibs', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.np_datetime', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.offsets', 'pandas._libs.tslibs.ccalendar', 'pandas._libs.tslibs.strptime', 'pandas._libs.tslibs.timezones', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.frequencies', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.fields', 'pandas._libs.tslibs.resolution', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.lib', 'fractions', 'pandas._libs.tslib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.config', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.printing', 'pandas.core.dtypes', 'pandas.core.dtypes.inference', 'pandas.io.formats.console', 'pandas.io.formats.terminal', 'pandas.core.api', 'pandas.core.arrays', 'pandas.core.arrays.array_', 'pandas.core.dtypes.common', 'pandas._libs.algos', 'pandas.core.dtypes.dtypes', 'pandas._libs.interval', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.util', 'pandas.util._decorators', 'pandas._libs.properties', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.dtypes.cast', 'pandas.core.dtypes.missing', 'pandas.util._validators', 'pandas.core.ops', 'pandas._libs.ops', 'pandas.core.common', 'pandas.core.missing', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.algorithms', 'pandas.core.base', 'pandas.core.nanops', 'pandas.tseries', 'pandas.tseries.offsets', 'dateutil.easter', 'pandas.core.tools', 'pandas.core.tools.datetimes', 'pandas.core.sorting', 'pandas.core.arrays.datetimes', 'pandas.core.arrays.datetimelike', 'pandas.tseries.frequencies', 'pandas.core.arrays._ranges', 'pandas.core.arrays.interval', 'pandas.util._doctools', 'pandas.core.indexes', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.join', 'pandas.core.dtypes.concat', 'pandas.core.indexes.frozen', 'pandas.core.strings', 'pandas.core.arrays.period', 'pandas.core.arrays.timedeltas', 'pandas.core.arrays.integer', 'pandas.core.tools.numeric', 'pandas.core.arrays.sparse', 'pandas._libs.sparse', 'pandas.core.arrays.numpy_', 'pandas.core.groupby', 'pandas.core.groupby.groupby', 'pandas._libs.groupby', 'pandas.core.frame', 'pandas.core.generic', 'pandas.core.index', 'pandas.core.indexes.api', 'pandas.core.indexes.category', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.datetimelike', 'pandas.core.tools.timedeltas', 'pandas.core.indexes.numeric', 'pandas.core.indexes.interval', 'pandas.util._exceptions', 'pandas.core.indexes.multi', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.period', 'pandas.core.indexes.range', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas.core.internals.blocks', 'pandas._libs.internals', 'pandas.core.internals.arrays', 'pandas.core.internals.managers', 'pandas.core.internals.concat', 'pandas.io.formats.format', 'pandas.io.common', 'mmap', 'pandas.core.internals.construction', 'pandas.core.series', 'pandas.core.indexes.accessors', 'pandas.plotting', 'pandas.plotting._misc', 'pandas.plotting._style', 'pandas.plotting._tools', 'pandas.plotting._core', 'pandas.plotting._compat', 'pandas.plotting._converter', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version']\n"
     ]
    }
   ],
   "source": [
    "# importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# creating a pandas dataframe of our json file \n",
    "sun_df=pd.read_json('sunrise_sunset.json', orient='records')\n",
    "sun_df['latitude'] = latitude_in_download_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>astronomical_twilight_begin</th>\n",
       "      <th>astronomical_twilight_end</th>\n",
       "      <th>civil_twilight_begin</th>\n",
       "      <th>civil_twilight_end</th>\n",
       "      <th>day_length</th>\n",
       "      <th>nautical_twilight_begin</th>\n",
       "      <th>nautical_twilight_end</th>\n",
       "      <th>solar_noon</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4:58:20 AM</td>\n",
       "      <td>7:29:49 PM</td>\n",
       "      <td>5:48:51 AM</td>\n",
       "      <td>6:39:18 PM</td>\n",
       "      <td>12:07:09</td>\n",
       "      <td>5:23:37 AM</td>\n",
       "      <td>7:04:32 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>6:10:30 AM</td>\n",
       "      <td>6:17:39 PM</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4:43:15 AM</td>\n",
       "      <td>7:44:54 PM</td>\n",
       "      <td>5:35:24 AM</td>\n",
       "      <td>6:52:45 PM</td>\n",
       "      <td>12:33:09</td>\n",
       "      <td>5:09:28 AM</td>\n",
       "      <td>7:18:41 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>5:57:30 AM</td>\n",
       "      <td>6:30:39 PM</td>\n",
       "      <td>10.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4:24:02 AM</td>\n",
       "      <td>8:04:07 PM</td>\n",
       "      <td>5:20:05 AM</td>\n",
       "      <td>7:08:04 PM</td>\n",
       "      <td>13:01:13</td>\n",
       "      <td>4:52:23 AM</td>\n",
       "      <td>7:35:46 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>5:43:28 AM</td>\n",
       "      <td>6:44:41 PM</td>\n",
       "      <td>20.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2:04:16 AM</td>\n",
       "      <td>10:23:52 PM</td>\n",
       "      <td>3:59:55 AM</td>\n",
       "      <td>8:28:14 PM</td>\n",
       "      <td>15:11:35</td>\n",
       "      <td>3:09:23 AM</td>\n",
       "      <td>9:18:46 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>4:38:17 AM</td>\n",
       "      <td>7:49:52 PM</td>\n",
       "      <td>50.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3:18:48 AM</td>\n",
       "      <td>9:09:21 PM</td>\n",
       "      <td>4:36:44 AM</td>\n",
       "      <td>7:51:25 PM</td>\n",
       "      <td>14:14:25</td>\n",
       "      <td>3:59:34 AM</td>\n",
       "      <td>8:28:35 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>5:06:52 AM</td>\n",
       "      <td>7:21:17 PM</td>\n",
       "      <td>40.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3:57:57 AM</td>\n",
       "      <td>8:30:12 PM</td>\n",
       "      <td>5:01:26 AM</td>\n",
       "      <td>7:26:43 PM</td>\n",
       "      <td>13:33:39</td>\n",
       "      <td>4:30:25 AM</td>\n",
       "      <td>7:57:44 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>5:27:15 AM</td>\n",
       "      <td>7:00:54 PM</td>\n",
       "      <td>30.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>2:51:05 AM</td>\n",
       "      <td>9:37:04 PM</td>\n",
       "      <td>16:46:55</td>\n",
       "      <td>12:43:53 AM</td>\n",
       "      <td>11:44:16 PM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>3:50:37 AM</td>\n",
       "      <td>8:37:32 PM</td>\n",
       "      <td>60.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>21:02:05</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:00:01 AM</td>\n",
       "      <td>12:14:04 PM</td>\n",
       "      <td>1:43:02 AM</td>\n",
       "      <td>10:45:07 PM</td>\n",
       "      <td>70.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  astronomical_twilight_begin astronomical_twilight_end civil_twilight_begin  \\\n",
       "0                  4:58:20 AM                7:29:49 PM           5:48:51 AM   \n",
       "1                  4:43:15 AM                7:44:54 PM           5:35:24 AM   \n",
       "2                  4:24:02 AM                8:04:07 PM           5:20:05 AM   \n",
       "3                  2:04:16 AM               10:23:52 PM           3:59:55 AM   \n",
       "4                  3:18:48 AM                9:09:21 PM           4:36:44 AM   \n",
       "5                  3:57:57 AM                8:30:12 PM           5:01:26 AM   \n",
       "6                 12:00:01 AM               12:00:01 AM           2:51:05 AM   \n",
       "7                 12:00:01 AM               12:00:01 AM          12:00:01 AM   \n",
       "\n",
       "  civil_twilight_end day_length nautical_twilight_begin nautical_twilight_end  \\\n",
       "0         6:39:18 PM   12:07:09              5:23:37 AM            7:04:32 PM   \n",
       "1         6:52:45 PM   12:33:09              5:09:28 AM            7:18:41 PM   \n",
       "2         7:08:04 PM   13:01:13              4:52:23 AM            7:35:46 PM   \n",
       "3         8:28:14 PM   15:11:35              3:09:23 AM            9:18:46 PM   \n",
       "4         7:51:25 PM   14:14:25              3:59:34 AM            8:28:35 PM   \n",
       "5         7:26:43 PM   13:33:39              4:30:25 AM            7:57:44 PM   \n",
       "6         9:37:04 PM   16:46:55             12:43:53 AM           11:44:16 PM   \n",
       "7        12:00:01 AM   21:02:05             12:00:01 AM           12:00:01 AM   \n",
       "\n",
       "    solar_noon     sunrise       sunset  latitude  \n",
       "0  12:14:04 PM  6:10:30 AM   6:17:39 PM      0.01  \n",
       "1  12:14:04 PM  5:57:30 AM   6:30:39 PM     10.01  \n",
       "2  12:14:04 PM  5:43:28 AM   6:44:41 PM     20.01  \n",
       "3  12:14:04 PM  4:38:17 AM   7:49:52 PM     50.01  \n",
       "4  12:14:04 PM  5:06:52 AM   7:21:17 PM     40.01  \n",
       "5  12:14:04 PM  5:27:15 AM   7:00:54 PM     30.01  \n",
       "6  12:14:04 PM  3:50:37 AM   8:37:32 PM     60.01  \n",
       "7  12:14:04 PM  1:43:02 AM  10:45:07 PM     70.01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing our dataframe\n",
    "sun_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-09 23:48:05 [matplotlib.axes._base] DEBUG: update_title_pos\n",
      "2019-05-09 23:48:05 [matplotlib.font_manager] DEBUG: findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/anaconda3/envs/first_sandbox/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "2019-05-09 23:48:05 [matplotlib.axes._base] DEBUG: update_title_pos\n",
      "2019-05-09 23:48:05 [matplotlib.axes._base] DEBUG: update_title_pos\n",
      "2019-05-09 23:48:05 [matplotlib.axes._base] DEBUG: update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting day-length versus latitude\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# getting day length into units of hours\n",
    "day_length_hours = []\n",
    "for time in sun_df['day_length']:\n",
    "    day_length_hours.append(int(time[0:2]) + int(time[3:5])/60 + int(time[6:8])/3600)\n",
    "\n",
    "\n",
    "plt.bar(height = list(day_length_hours), x = list(sun_df['latitude']))\n",
    "plt.title('Day Length vs Latitude')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
